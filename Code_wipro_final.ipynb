{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4253e3d3a0914818a481840e8d1f576b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d9fdcbb8708b42928a21c65df94037dc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_74be07128244435c8e4f1d03beff79f1",
              "IPY_MODEL_cf5da7d42b5e400bb75254cb03b5424f",
              "IPY_MODEL_3ca1232969a54875810a4aa8f84d2c36"
            ]
          }
        },
        "d9fdcbb8708b42928a21c65df94037dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "74be07128244435c8e4f1d03beff79f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1f1dd9e543d547c5a631542308c00f95",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Testing: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3cc7107e0a0f4de9ac62bc9384b14d98"
          }
        },
        "cf5da7d42b5e400bb75254cb03b5424f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ed2bee8f5d904cde8357cded2a86ed89",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_905388e2316c47ae8a66944d791c9338"
          }
        },
        "3ca1232969a54875810a4aa8f84d2c36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1dbb901316634bec8f9725f985bb6385",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00,  4.76it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_19d62ae3d3d94c168ed849fc2a867802"
          }
        },
        "1f1dd9e543d547c5a631542308c00f95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3cc7107e0a0f4de9ac62bc9384b14d98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ed2bee8f5d904cde8357cded2a86ed89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "905388e2316c47ae8a66944d791c9338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1dbb901316634bec8f9725f985bb6385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "19d62ae3d3d94c168ed849fc2a867802": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install NRCLex\n",
        "!python -m textblob.download_corpora\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n"
      ],
      "metadata": {
        "id": "K8z4yJkUnSCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5TP-Sv_TnTUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#change the file path according to your drive\n",
        "data = pd.read_csv('/content/drive/MyDrive/HELIOS_MATERIAL_IIT_PATNA/MEMES/memes_our_dataset_hindi/MEMES_MY_DATASET_WITHOUT_OVERSAMPLING_new.csv')"
      ],
      "metadata": {
        "id": "hniPhkaHnWSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import clip\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "08h0pFjenaD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/FreddeFrallan/Multilingual-CLIP\n",
        "!bash Multilingual-CLIP/get-weights.sh\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "SN2OUFSynfGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(data):\n",
        "  #data = pd.read_csv(dataset_path)\n",
        "  text = list(data['text'])\n",
        "  img_path = list(data['Name'])\n",
        "  label = list(data['Level1'])\n",
        "  valence = list(data['Valence'])\n",
        "  valence = list(map(lambda x: x - 1 , valence))\n",
        "  arousal = list(data['Arousal'])\n",
        "  arousal = list(map(lambda x: x - 1 , arousal))\n",
        "  #optimize memory for features\n",
        "  text_features,image_features,l,a,v = [],[],[],[],[]\n",
        "  for txt,img,L,A,V in tqdm(zip(text,img_path,label,arousal,)):\n",
        "\n",
        "    try:\n",
        "      #img = preprocess(Image.open('/content/drive/.shortcut-targets-by-id/1Z57L19m3ZpJ6bEPdyaIMYuI00Tc2RT1I/memes_our_dataset_hindi/my_meme_data/'+img)).unsqueeze(0).to(device)\n",
        "      img = Image.open('/content/drive/.shortcut-targets-by-id/1Z57L19m3ZpJ6bEPdyaIMYuI00Tc2RT1I/memes_our_dataset_hindi/my_meme_data/'+img)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      continue\n",
        "\n",
        "    img = torch.stack([compose(img).to(device)])\n",
        "    l.append(L)\n",
        "    a.append(A)\n",
        "    v.append(V)\n",
        "    #txt = torch.as_tensor(txt)\n",
        "    with torch.no_grad():\n",
        "      temp_txt = text_model([txt]).detach().cpu().numpy()\n",
        "      text_features.append(temp_txt)\n",
        "      temp_img = clip_model.encode_image(img).detach().cpu().numpy()\n",
        "      image_features.append(temp_img)\n",
        "\n",
        "      del temp_txt\n",
        "      del temp_img\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    del img\n",
        "    #del txt\n",
        "    torch.cuda.empty_cache()\n",
        "  return text_features,image_features,l,v,a"
      ],
      "metadata": {
        "id": "sqIxYeBBnoWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HatefulDataset(Dataset):\n",
        "\n",
        "  def __init__(self,data):\n",
        "\n",
        "    self.t_f,self.i_f,self.label,self.v,self.a,self.fe, self.neg, self.ir, self.ra, \\\n",
        "    self.disg, self.ner, self.sh, self.disa, self.en, self.su, self.sa, self.jo, self.pr, \\\n",
        "    self.sar, self.hum, self.inten, self.t1, self.t2, self.t3, \\\n",
        "    self.t4, self.t5, self.t6, self.t7 = get_data(data)\n",
        "    self.t_f = np.squeeze(np.asarray(self.t_f),axis=1)\n",
        "    self.i_f = np.squeeze(np.asarray(self.i_f),axis=1)\n",
        "\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.a)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    #print(idx)\n",
        "    label = self.label[idx]\n",
        "    T = self.t_f[idx,:]\n",
        "    I = self.i_f[idx,:]\n",
        "    v = self.v[idx]\n",
        "    a = self.a[idx]\n",
        "    fe = self.fe[idx]\n",
        "    neg = self.neg[idx]\n",
        "    ir = self.ir[idx]\n",
        "    ra = self.ra[idx]\n",
        "    disg = self.disg[idx]\n",
        "    ner = self.ner[idx]\n",
        "    sh = self.sh[idx]\n",
        "    disa = self.disa[idx]\n",
        "    en = self.en[idx]\n",
        "    su = self.su[idx]\n",
        "    sa = self.sa[idx]\n",
        "    jo = self.jo[idx]\n",
        "    pr = self.pr[idx]\n",
        "    sar = self.sar[idx]\n",
        "    hum = self.hum[idx]\n",
        "    inten = self.inten[idx]\n",
        "    t1 = self.t1[idx]\n",
        "    t2 = self.t2[idx]\n",
        "    t3 = self.t3[idx]\n",
        "    t4 = self.t4[idx]\n",
        "    t5 = self.t5[idx]\n",
        "    t6 = self.t6[idx]\n",
        "    t7 = self.t7[idx]\n",
        "    #name = self.name[idx]\n",
        "\n",
        "    sample = {'label':label,'processed_txt':T,'processed_img':I,'valence':v,'arousal':a , 'fear': fe, 'neglect': neg, \\\n",
        "              'irritation':ir, 'rage':ra, 'disgust':disg, 'nervousness':ner, 'shame':sh, 'disappointment':disa, \\\n",
        "              'envy':en, 'suffering':su, 'sadness':sa, 'joy':jo, 'pride':pr, 'sarcasm':sar, 'humor': hum, \\\n",
        "              'inten': inten, 't1':t1, 't2':t2, 't3':t3, 't4':t4, 't5':t5, 't6':t6, 't7': t7}\n",
        "    return sample\n",
        "\n"
      ],
      "metadata": {
        "id": "qMeqNDtlnwOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers=['test25.png',\n",
        " 'test31.png',\n",
        " 'hin32.png',\n",
        " 'mix_meme7.png',\n",
        " 'chaukidar37.png',\n",
        " 'chaukidar43.png',\n",
        " 'gandhiji77.png',\n",
        " 'politics363.png',\n",
        " 'rel563.png',\n",
        " 'match251.png',\n",
        " 'kalam360.png',\n",
        " 'file_new_5.png',\n",
        " 'file_new_426.png',\n",
        " 'ravan2.png',\n",
        " 'ravan20.png',\n",
        " 'ravan23.png',\n",
        " 'ravan24.png',\n",
        " 'ravan25.png',\n",
        " 'ravan26.png',\n",
        " 'ravan38.png',\n",
        " 'ravan282.png',\n",
        " 'ravan283.png',\n",
        " 'ravan284.png',\n",
        " 'ravan296.png',\n",
        " 'ravan299.png',\n",
        " 'ravan341.png',\n",
        " 'ravan342.png',\n",
        " 'ravan343.png',\n",
        " 'ravan344.png',\n",
        " 'm_50.png']"
      ],
      "metadata": {
        "id": "wDwZVUqwn_r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HatefulDatasetFinal(Dataset):\n",
        "\n",
        "  def __init__(self,data,dataset,outliers):\n",
        "\n",
        "    self.name, self.t_f,self.i_f,self.label,self.v,self.a,self.fe, self.neg, self.ir, self.ra, \\\n",
        "    self.disg, self.ner, self.sh, self.disa, self.en, self.su, self.sa, self.jo, self.pr, \\\n",
        "    self.sar, self.hum, self.inten, self.t1, self.t2, self.t3, \\\n",
        "    self.t4, self.t5, self.t6, self.t7 = \\\n",
        "    list(data['Name']), \\\n",
        "    [i['processed_txt'] for i in dataset], \\\n",
        "    [i['processed_img'] for i in dataset], \\\n",
        "    [i['label'] for i in dataset], \\\n",
        "    [i['valence'] for i in dataset], \\\n",
        "    [i['arousal'] for i in dataset], \\\n",
        "    [i['fear'] for i in dataset], \\\n",
        "    [i['neglect'] for i in dataset], \\\n",
        "    [i['irritation'] for i in dataset], \\\n",
        "    [i['rage'] for i in dataset], \\\n",
        "    [i['disgust'] for i in dataset], \\\n",
        "    [i['nervousness'] for i in dataset], \\\n",
        "    [i['shame'] for i in dataset], \\\n",
        "    [i['disappointment'] for i in dataset], \\\n",
        "    [i['envy'] for i in dataset], \\\n",
        "    [i['suffering'] for i in dataset], \\\n",
        "    [i['sadness'] for i in dataset], \\\n",
        "    [i['joy'] for i in dataset], \\\n",
        "    [i['pride'] for i in dataset], \\\n",
        "    [i['sarcasm'] for i in dataset], \\\n",
        "    [i['humor'] for i in dataset], \\\n",
        "    [i['inten'] for i in dataset], \\\n",
        "    [i['t1'] for i in dataset], \\\n",
        "    [i['t2'] for i in dataset], \\\n",
        "    [i['t3'] for i in dataset], \\\n",
        "    [i['t4'] for i in dataset], \\\n",
        "    [i['t5'] for i in dataset], \\\n",
        "    [i['t6'] for i in dataset], \\\n",
        "    [i['t7'] for i in dataset]\n",
        "\n",
        "    self.t_f = np.asarray(self.t_f)\n",
        "    self.i_f = np.asarray(self.i_f)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.a)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    label = self.label[idx]\n",
        "    T = self.t_f[idx,:]\n",
        "    I = self.i_f[idx,:]\n",
        "    v = self.v[idx]\n",
        "    a = self.a[idx]\n",
        "    fe = self.fe[idx]\n",
        "    neg = self.neg[idx]\n",
        "    ir = self.ir[idx]\n",
        "    ra = self.ra[idx]\n",
        "    disg = self.disg[idx]\n",
        "    ner = self.ner[idx]\n",
        "    sh = self.sh[idx]\n",
        "    disa = self.disa[idx]\n",
        "    en = self.en[idx]\n",
        "    su = self.su[idx]\n",
        "    sa = self.sa[idx]\n",
        "    jo = self.jo[idx]\n",
        "    pr = self.pr[idx]\n",
        "    sar = self.sar[idx]\n",
        "    hum = self.hum[idx]\n",
        "    inten = self.inten[idx]\n",
        "    t1 = self.t1[idx]\n",
        "    t2 = self.t2[idx]\n",
        "    t3 = self.t3[idx]\n",
        "    t4 = self.t4[idx]\n",
        "    t5 = self.t5[idx]\n",
        "    t6 = self.t6[idx]\n",
        "    t7 = self.t7[idx]\n",
        "    name = self.name[idx]\n",
        "\n",
        "    sample = {'name':name, 'label':label,'processed_txt':T,'processed_img':I,'valence':v,'arousal':a , 'fear': fe, 'neglect': neg, \\\n",
        "              'irritation':ir, 'rage':ra, 'disgust':disg, 'nervousness':ner, 'shame':sh, 'disappointment':disa, \\\n",
        "              'envy':en, 'suffering':su, 'sadness':sa, 'joy':jo, 'pride':pr, 'sarcasm':sar, 'humor': hum, \\\n",
        "              'inten': inten, 't1':t1, 't2':t2, 't3':t3, 't4':t4, 't5':t5, 't6':t6, 't7': t7}\n",
        "    return sample\n",
        "\n"
      ],
      "metadata": {
        "id": "kUqoBiBDoInK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning\n",
        "import pytorch_lightning as pl\n"
      ],
      "metadata": {
        "id": "SPxv3P_YoOIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class MFB(nn.Module):\n",
        "    def __init__(self,img_feat_size, ques_feat_size, is_first, MFB_K, MFB_O, DROPOUT_R):\n",
        "        super(MFB, self).__init__()\n",
        "        #self.__C = __C\n",
        "        self.MFB_K = MFB_K\n",
        "        self.MFB_O = MFB_O\n",
        "        self.DROPOUT_R = DROPOUT_R\n",
        "\n",
        "        self.is_first = is_first\n",
        "        self.proj_i = nn.Linear(img_feat_size, MFB_K * MFB_O)\n",
        "        self.proj_q = nn.Linear(ques_feat_size, MFB_K * MFB_O)\n",
        "\n",
        "        self.dropout = nn.Dropout(DROPOUT_R)\n",
        "        self.pool = nn.AvgPool1d(MFB_K, stride = MFB_K)\n",
        "\n",
        "    def forward(self, img_feat, ques_feat, exp_in=1):\n",
        "        '''\n",
        "            img_feat.size() -> (N, C, img_feat_size)    C = 1 or 100\n",
        "            ques_feat.size() -> (N, 1, ques_feat_size)\n",
        "            z.size() -> (N, C, MFB_O)\n",
        "            exp_out.size() -> (N, C, K*O)\n",
        "        '''\n",
        "        batch_size = img_feat.shape[0]\n",
        "        img_feat = self.proj_i(img_feat)                # (N, C, K*O)\n",
        "        ques_feat = self.proj_q(ques_feat)              # (N, 1, K*O)\n",
        "\n",
        "        exp_out = img_feat * ques_feat             # (N, C, K*O)\n",
        "        exp_out = self.dropout(exp_out) if self.is_first else self.dropout(exp_out * exp_in)     # (N, C, K*O)\n",
        "        z = self.pool(exp_out) * self.MFB_K         # (N, C, O)\n",
        "        z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))\n",
        "        z = F.normalize(z.view(batch_size, -1))         # (N, C*O)\n",
        "        z = z.view(batch_size, -1, self.MFB_O)      # (N, C, O)\n",
        "        return z"
      ],
      "metadata": {
        "id": "m_HUXP-FqOTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hm_final = torch.load('/content/drive/MyDrive/HELIOS_MATERIAL_IIT_PATNA/MEMES/memes_our_dataset_hindi/hm_data_hard_new.pt')\n",
        "hm_final = HatefulDatasetFinal(data,hm_final,outliers)\n",
        "torch.manual_seed(123)\n",
        "t_p,te_p = torch.utils.data.random_split(hm_final,[5908,1478])\n",
        "torch.manual_seed(123)\n",
        "t_p,v_p = torch.utils.data.random_split(t_p,[5022,886])"
      ],
      "metadata": {
        "id": "qgH598QspmM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(t_p[4])"
      ],
      "metadata": {
        "id": "ZQ1aXsiD6fR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score,precision_score\n",
        "import torch\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import os\n",
        "import warnings\n",
        "import torch\n",
        "#t_m = HatefulDatasetMemotion('/content/train/train_data.csv')"
      ],
      "metadata": {
        "id": "FAlns69_rAwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "o_p,e1_p,e2_p,e3_p,e4_p,e5_p,e6_p,e7_p,e8_p,e9_p,e10_p,e11_p,e12_p,e13_p,i_p = \\\n",
        "[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
        "o_t,e1_t,e2_t,e3_t,e4_t,e5_t,e6_t,e7_t,e8_t,e9_t,e10_t,e11_t,e12_t,e13_t,i_t = \\\n",
        "[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
        "def append_p(tba,appendee):\n",
        "  for i in np.argmax(tba.detach().cpu().numpy(),axis=-1):\n",
        "    appendee.append(i)\n",
        "def append_gt(tba,appendee):\n",
        "  for i in tba.detach().cpu().numpy():\n",
        "    appendee.append(i)\n",
        "N = []"
      ],
      "metadata": {
        "id": "4kZpwhwxrGt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_e = 0\n",
        "import torch\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score,precision_score\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import os\n",
        "\n",
        "class Classifier(pl.LightningModule):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.MFB = MFB(640,640,True,256,64,0.1)\n",
        "    self.loss_fn_emotion=torch.nn.KLDivLoss(reduction='batchmean',log_target=True)\n",
        "    self.encode_text = torch.nn.Linear(1280,64)\n",
        "    self.fin = torch.nn.Linear(64,2)\n",
        "    self.fin_v = torch.nn.Linear(64,4)\n",
        "    self.fin_a = torch.nn.Linear(64,4)\n",
        "    self.fin_sarcasm = torch.nn.Linear(64,3)\n",
        "    self.fin_e1 = torch.nn.Linear(64,2)\n",
        "    self.fin_e2 = torch.nn.Linear(64,2)\n",
        "    self.fin_e3 = torch.nn.Linear(64,2)\n",
        "    self.fin_e4 = torch.nn.Linear(64,2)\n",
        "    self.fin_e5 = torch.nn.Linear(64,2)\n",
        "    self.fin_e6 = torch.nn.Linear(64,2)\n",
        "    self.fin_e7 = torch.nn.Linear(64,2)\n",
        "    self.fin_e8 = torch.nn.Linear(64,2)\n",
        "    self.fin_e9 = torch.nn.Linear(64,2)\n",
        "    self.fin_e10 = torch.nn.Linear(64,2)\n",
        "    self.fin_e11 = torch.nn.Linear(64,2)\n",
        "    self.fin_e12 = torch.nn.Linear(64,2)\n",
        "    self.fin_e13 = torch.nn.Linear(64,2)\n",
        "    self.fin_inten = torch.nn.Linear(64,3)\n",
        "    self.fin_target_ident = torch.nn.Linear(64,7)\n",
        "    self.fin_emotion_mult = torch.nn.Linear(64,13)\n",
        "\n",
        "\n",
        "  def forward(self, x,y):\n",
        "\n",
        "      x_,y_ = x,y\n",
        "      x = x.float()\n",
        "      y = y.float()\n",
        "      z_ = self.MFB(torch.unsqueeze(y,axis=1),torch.unsqueeze(x,axis=1))\n",
        "      z = z_\n",
        "      c = self.fin(torch.squeeze(z,dim=1))\n",
        "      c_inten = self.fin_inten(torch.squeeze(z,dim=1))\n",
        "      c_v = self.fin_v(torch.squeeze(z,dim=1))\n",
        "      c_a = self.fin_a(torch.squeeze(z,dim=1))\n",
        "      c_e1 = self.fin_e1(torch.squeeze(z,dim=1))\n",
        "      c_e2 = self.fin_e2(torch.squeeze(z,dim=1))\n",
        "      c_e3 = self.fin_e3(torch.squeeze(z,dim=1))\n",
        "      c_e4 = self.fin_e4(torch.squeeze(z,dim=1))\n",
        "      c_e5 = self.fin_e5(torch.squeeze(z,dim=1))\n",
        "      c_e6 = self.fin_e6(torch.squeeze(z,dim=1))\n",
        "      c_e7 = self.fin_e7(torch.squeeze(z,dim=1))\n",
        "      c_e8 = self.fin_e8(torch.squeeze(z,dim=1))\n",
        "      c_e9 = self.fin_e9(torch.squeeze(z,dim=1))\n",
        "      c_e10 = self.fin_e10(torch.squeeze(z,dim=1))\n",
        "      c_e11 = self.fin_e11(torch.squeeze(z,dim=1))\n",
        "      c_e12 = self.fin_e12(torch.squeeze(z,dim=1))\n",
        "      c_e13 = self.fin_e13(torch.squeeze(z,dim=1))\n",
        "      c_sarcasm = self.fin_sarcasm(torch.squeeze(z,dim=1))\n",
        "      # probability distribution over labels\n",
        "      c = torch.log_softmax(c, dim=1)\n",
        "      c_inten = torch.log_softmax(c_inten, dim=1)\n",
        "      c_a = torch.log_softmax(c_a, dim=1)\n",
        "      c_v = torch.log_softmax(c_v, dim=1)\n",
        "      c_e1 = torch.log_softmax(c_e1, dim=1)\n",
        "      c_sarcasm = torch.log_softmax(c_sarcasm, dim=1)\n",
        "      c_e2 = torch.log_softmax(c_e2, dim=1)\n",
        "      c_e3 = torch.log_softmax(c_e3, dim=1)\n",
        "      c_e4 = torch.log_softmax(c_e4, dim=1)\n",
        "\n",
        "      c_e5 = torch.log_softmax(c_e5, dim=1)\n",
        "      c_e6 = torch.log_softmax(c_e6, dim=1)\n",
        "      c_e7 = torch.log_softmax(c_e7, dim=1)\n",
        "\n",
        "      c_e8 = torch.log_softmax(c_e8, dim=1)\n",
        "      c_e9 = torch.log_softmax(c_e9, dim=1)\n",
        "      c_e10 = torch.log_softmax(c_e10, dim=1)\n",
        "\n",
        "      c_e11 = torch.log_softmax(c_e11, dim=1)\n",
        "      c_e12 = torch.log_softmax(c_e12, dim=1)\n",
        "      c_e13 = torch.log_softmax(c_e13, dim=1)\n",
        "      c_target = self.fin_target_ident(torch.squeeze(z,dim=1))\n",
        "      c_emotion = self.fin_emotion_mult(torch.squeeze(z,dim=1))\n",
        "      return z,c,c_a,c_v,c_e1,c_e2,c_e3,c_e4,c_e5,c_e6,c_e7,c_e8,c_e9,c_e10,c_e11,c_e12, c_e13, c_inten, c_target, c_sarcasm, c_emotion\n",
        "\n",
        "  def cross_entropy_loss(self, logits, labels):\n",
        "    return F.nll_loss(logits, labels)\n",
        "\n",
        "\n",
        "  def training_step(self, train_batch, batch_idx):\n",
        "      _,lab,txt,img,val,arou,e1,e2,e3,e4,e5,e6,e7,e8,e9,e10,e11,e12,e13,sarcasm,_,intensity,t1,t2,t3,t4,t5,t6,t7 = train_batch\n",
        "\n",
        "      lab = train_batch[lab]\n",
        "      #print(lab)\n",
        "      txt = train_batch[txt]\n",
        "      #print(txt)\n",
        "\n",
        "      img = train_batch[img]\n",
        "      val = train_batch[val]\n",
        "      arou = train_batch[arou]\n",
        "      #print(a)\n",
        "      e1 = train_batch[e1]\n",
        "      e2 = train_batch[e2]\n",
        "      e3 = train_batch[e3]\n",
        "      e4 = train_batch[e4]\n",
        "      e5 = train_batch[e5]\n",
        "      e6 = train_batch[e6]\n",
        "      e7 = train_batch[e7]\n",
        "      e8 = train_batch[e8]\n",
        "      e9 = train_batch[e9]\n",
        "      e10 = train_batch[e10]\n",
        "      e11 = train_batch[e11]\n",
        "      e12 = train_batch[e12]\n",
        "      e13 = train_batch[e13]\n",
        "      intensity = train_batch[intensity]\n",
        "      sarcasm = train_batch[sarcasm]\n",
        "      t1,t2,t3,t4,t5,t6,t7 = torch.unsqueeze(train_batch[t1],1),torch.unsqueeze(train_batch[t2],1),\\\n",
        "      torch.unsqueeze(train_batch[t3],1),torch.unsqueeze(train_batch[t4],1),torch.unsqueeze(train_batch[t5],1),\\\n",
        "      torch.unsqueeze(train_batch[t6],1),torch.unsqueeze(train_batch[t7],1)\n",
        "      gt_target = torch.cat((t1,t2,t3,t4,t5,t6,t7),1) #ground truth target\n",
        "\n",
        "      gt_emotion = torch.cat((torch.unsqueeze(e1,1),torch.unsqueeze(e2,1),torch.unsqueeze(e3,1),torch.unsqueeze(e4,1),torch.unsqueeze(e5,1),torch.unsqueeze(e6,1),\\\n",
        "                              torch.unsqueeze(e7,1),torch.unsqueeze(e8,1),torch.unsqueeze(e9,1),torch.unsqueeze(e10,1),torch.unsqueeze(e11,1),torch.unsqueeze(e12,1),\\\n",
        "                              torch.unsqueeze(e13,1)),1)\n",
        "\n",
        "      z,logit_offen,logit_arou,logit_val, a,b,c,d,e,f,g,h,i,j,k,l,m,,logitinten_target,logit_sarcasm,logit_emotion = self.forward(txt,img) # logit_target is logits of target\n",
        "\n",
        "      loss1 = self.cross_entropy_loss(logit_offen, lab)\n",
        "      #loss2 = self.cross_entropy_loss(logit_arou, arou)\n",
        "      #loss3 = self.cross_entropy_loss(logit_val, val)\n",
        "      loss4 = self.cross_entropy_loss(a, e1)\n",
        "      loss5 = self.cross_entropy_loss(b, e2)\n",
        "      loss6 = self.cross_entropy_loss(c, e3)\n",
        "      loss7 = self.cross_entropy_loss(d, e4)\n",
        "      loss8 = self.cross_entropy_loss(e, e5)\n",
        "      loss9 = self.cross_entropy_loss(f, e6)\n",
        "      loss10 = self.cross_entropy_loss(g, e7)\n",
        "      loss11 = self.cross_entropy_loss(h, e8)\n",
        "      loss12 = self.cross_entropy_loss(i, e9)\n",
        "      loss13 = self.cross_entropy_loss(j, e10)\n",
        "      loss14 = self.cross_entropy_loss(k, e11)\n",
        "      loss15 = self.cross_entropy_loss(l, e12)\n",
        "      loss16 = self.cross_entropy_loss(m, e13)\n",
        "      loss17 = self.cross_entropy_loss(inten, intensity)\n",
        "\n",
        "      loss18 = F.binary_cross_entropy_with_logits(logit_target.float(), gt_target.float())\n",
        "      loss_emo_mult = F.binary_cross_entropy_with_logits(logit_emotion.float(), gt_emotion.float())\n",
        "      loss_sarcasm = self.cross_entropy_loss(logit_sarcasm, sarcasm)\n",
        "\n",
        "      #loss = loss1+loss_emo_mult+loss17\n",
        "      loss=loss1+loss_emo_mult\n",
        "      self.log('train_loss', loss)\n",
        "\n",
        "      return loss\n",
        "\n",
        "\n",
        "  def validation_step(self, val_batch, batch_idx):\n",
        "      lab,txt,img,e1,e2,e3,e4,e5,e6,e7,e8,e9,e10,e11,e12,e13,intensity,t1,t2,t3,t4,t5,t6 = val_batch\n",
        "      #print(val_batch)\n",
        "      lab = val_batch[lab]\n",
        "      txt = val_batch[txt]\n",
        "      img = val_batch[img]\n",
        "      val = val_batch[val]\n",
        "      arou = val_batch[arou]\n",
        "      e1 = val_batch[e1]\n",
        "      e2 = val_batch[e2]\n",
        "      e3 = val_batch[e3]\n",
        "      e4 = val_batch[e4]\n",
        "      e5 = val_batch[e5]\n",
        "      e6 = val_batch[e6]\n",
        "      e7 = val_batch[e7]\n",
        "      e8 = val_batch[e8]\n",
        "      e9 = val_batch[e9]\n",
        "      e10 = val_batch[e10]\n",
        "      e11 = val_batch[e11]\n",
        "      e12 = val_batch[e12]\n",
        "      e13 = val_batch[e13]\n",
        "      sarcasm = val_batch[sarcasm]\n",
        "      intensity = val_batch[intensity]\n",
        "      t1,t2,t3,t4,t5,t6,t7 = torch.unsqueeze(val_batch[t1],1),torch.unsqueeze(val_batch[t2],1),\\\n",
        "      torch.unsqueeze(val_batch[t3],1),torch.unsqueeze(val_batch[t4],1),torch.unsqueeze(val_batch[t5],1),\\\n",
        "      torch.unsqueeze(val_batch[t6],1),torch.unsqueeze(val_batch[t7],1)\n",
        "      #print(t1.size())\n",
        "      gt_target = torch.cat((t1,t2,t3,t4,t5,t6,t7),1) #ground truth target\n",
        "      gt_emotion = torch.cat((torch.unsqueeze(e1,1),torch.unsqueeze(e2,1),torch.unsqueeze(e3,1),torch.unsqueeze(e4,1),torch.unsqueeze(e5,1),torch.unsqueeze(e6,1),\\\n",
        "                              torch.unsqueeze(e7,1),torch.unsqueeze(e8,1),torch.unsqueeze(e9,1),torch.unsqueeze(e10,1),torch.unsqueeze(e11,1),torch.unsqueeze(e12,1),\\\n",
        "                              torch.unsqueeze(e13,1)),1)\n",
        "\n",
        "      _,logits,logit_arou,logit_val, a,b,c,d,e,f,g,h,i,j,k,l,m,inten,logit_target,logit_sarcasm,logit_emotion = self.forward(txt,img)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      tmp = np.argmax(logits.detach().cpu().numpy(),axis=-1)\n",
        "      loss = self.cross_entropy_loss(logits, lab)\n",
        "      lab = lab.detach().cpu().numpy()\n",
        "      self.log('val_acc', f1_score(lab,tmp,average='macro'))\n",
        "      #self.log('val_roc_auc',roc_auc_score(lab,tmp))\n",
        "      self.log('val_loss', loss)\n",
        "      tqdm_dict = {'val_acc': accuracy_score(lab,tmp)}\n",
        "      #print('Val acc {}'.format(accuracy_score(lab,tmp)))\n",
        "      return {\n",
        "                'progress_bar': tqdm_dict,\n",
        "              'val_loss_target': F.binary_cross_entropy_with_logits(logit_target.float(), gt_target.float()),\n",
        "              'val_loss_emotion_multilabel': F.binary_cross_entropy_with_logits(logit_emotion.float(), gt_emotion.float()),\n",
        "              'val_acc e1': accuracy_score(e1.detach().cpu().numpy(),np.argmax(a.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e2': accuracy_score(e2.detach().cpu().numpy(),np.argmax(b.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e3': accuracy_score(e3.detach().cpu().numpy(),np.argmax(c.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e4': accuracy_score(e4.detach().cpu().numpy(),np.argmax(d.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e5': accuracy_score(e5.detach().cpu().numpy(),np.argmax(e.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e6': accuracy_score(e6.detach().cpu().numpy(),np.argmax(f.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e7': accuracy_score(e7.detach().cpu().numpy(),np.argmax(g.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e8': accuracy_score(e8.detach().cpu().numpy(),np.argmax(h.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e9': accuracy_score(e9.detach().cpu().numpy(),np.argmax(i.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e10': accuracy_score(e10.detach().cpu().numpy(),np.argmax(j.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e11': accuracy_score(e11.detach().cpu().numpy(),np.argmax(k.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e12': accuracy_score(e12.detach().cpu().numpy(),np.argmax(l.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e13': accuracy_score(e13.detach().cpu().numpy(),np.argmax(m.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc intensity': f1_score(intensity.detach().cpu().numpy(),np.argmax(inten.detach().cpu().numpy(),axis=-1),average='macro'),\n",
        "       'val_acc sarcasm': accuracy_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1)),\n",
        "       'f1 sarcasm': f1_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1),average='macro')\n",
        "      }\n",
        "\n",
        "  def validation_epoch_end(self, validation_step_outputs):\n",
        "    outs = []\n",
        "    outs1,outs2,outs3,outs4,outs5,outs6,outs7,outs8,outs9,outs10,outs11,outs12,outs13,outs14,outs16,outs17 = \\\n",
        "    [],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
        "    outs15 = []\n",
        "    outs18 = []\n",
        "    for out in validation_step_outputs:\n",
        "      outs.append(out['progress_bar']['val_acc'])\n",
        "      outs1.append(out['val_acc e1'])\n",
        "      outs2.append(out['val_acc e2'])\n",
        "      outs3.append(out['val_acc e3'])\n",
        "      outs4.append(out['val_acc e4'])\n",
        "      outs5.append(out['val_acc e5'])\n",
        "      outs6.append(out['val_acc e6'])\n",
        "      outs7.append(out['val_acc e7'])\n",
        "      outs8.append(out['val_acc e8'])\n",
        "      outs9.append(out['val_acc e9'])\n",
        "      outs10.append(out['val_acc e10'])\n",
        "      outs11.append(out['val_acc e11'])\n",
        "      outs12.append(out['val_acc e12'])\n",
        "      outs13.append(out['val_acc e13'])\n",
        "      outs14.append(out['val_acc intensity'])\n",
        "      outs15.append(out['val_loss_target'])\n",
        "      outs16.append(out['val_loss_emotion_multilabel'])\n",
        "      outs17.append(out['val_acc sarcasm'])\n",
        "      outs18.append(out['f1 sarcasm'])\n",
        "    self.log('val_acc_all_offn', sum(outs)/len(outs))\n",
        "    self.log('val_loss_target', sum(outs15)/len(outs15))\n",
        "    self.log('val_acc_all e1', sum(outs1)/len(outs1))\n",
        "    self.log('val_acc_all e2', sum(outs2)/len(outs2))\n",
        "    self.log('val_acc_all e3', sum(outs3)/len(outs3))\n",
        "    self.log('val_acc_all e4', sum(outs4)/len(outs4))\n",
        "    self.log('val_acc_all e5', sum(outs5)/len(outs5))\n",
        "    self.log('val_acc_all e6', sum(outs6)/len(outs6))\n",
        "    self.log('val_acc_all e7', sum(outs7)/len(outs7))\n",
        "    self.log('val_acc_all e8', sum(outs8)/len(outs8))\n",
        "    self.log('val_acc_all e9', sum(outs9)/len(outs9))\n",
        "    self.log('val_acc_all e10', sum(outs10)/len(outs10))\n",
        "    self.log('val_acc_all e11', sum(outs11)/len(outs11))\n",
        "    self.log('val_acc_all e12', sum(outs12)/len(outs12))\n",
        "    self.log('val_acc_all e13', sum(outs13)/len(outs13))\n",
        "    self.log('val_acc_all inten', sum(outs14)/len(outs14))\n",
        "    self.log('val_loss_all emo', sum(outs16)/len(outs16))\n",
        "    self.log('val_acc_all sarcasm', sum(outs17)/len(outs17))\n",
        "    self.log('val_f1_all sarcasm', sum(outs18)/len(outs18))\n",
        "\n",
        "    print(f'***offensive f1 at epoch end {sum(outs)/len(outs)}****')\n",
        "    #print(f'***val acc inten at epoch end {sum(outs14)/len(outs14)}****')\n",
        "    print(f'***val loss emotion at epoch end {sum(outs16)/len(outs16)}****')\n",
        "    #print(f'***val acc sarcasm at epoch end {sum(outs17)/len(outs17)}****')\n",
        "    #print(f'***val f1 sarcasm at epoch end {sum(outs18)/len(outs18)}****')\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "      name,lab,txt,img,val,arou,e1,e2,e3,e4,e5,e6,e7,e8,e9,e10,e11,e12,e13,sarcasm,_,intensity,t1,t2,t3,t4,t5,t6,t7 = batch\n",
        "      name = batch[name]\n",
        "      lab = batch[lab]\n",
        "      txt = batch[txt]\n",
        "      img = batch[img]\n",
        "      e1 = batch[e1]\n",
        "      e2 = batch[e2]\n",
        "      e3 = batch[e3]\n",
        "      e4 = batch[e4]\n",
        "      e5 = batch[e5]\n",
        "      e6 = batch[e6]\n",
        "      e7 = batch[e7]\n",
        "      e8 = batch[e8]\n",
        "      e9 = batch[e9]\n",
        "      e10 = batch[e10]\n",
        "      e11 = batch[e11]\n",
        "      e12 = batch[e12]\n",
        "      e13 = batch[e13]\n",
        "      intensity = batch[intensity]\n",
        "      sarcasm = batch[sarcasm]\n",
        "      t1,t2,t3,t4,t5,t6,t7 = torch.unsqueeze(batch[t1],1),torch.unsqueeze(batch[t2],1),\\\n",
        "      torch.unsqueeze(batch[t3],1),torch.unsqueeze(batch[t4],1),torch.unsqueeze(batch[t5],1),\\\n",
        "      torch.unsqueeze(batch[t6],1),torch.unsqueeze(batch[t7],1)\n",
        "      gt_target = torch.cat((t1,t2,t3,t4,t5,t6,t7),1)\n",
        "      gt_emotion = torch.cat((torch.unsqueeze(e1,1),torch.unsqueeze(e2,1),torch.unsqueeze(e3,1),torch.unsqueeze(e4,1),torch.unsqueeze(e5,1),torch.unsqueeze(e6,1),\\\n",
        "                              torch.unsqueeze(e7,1),torch.unsqueeze(e8,1),torch.unsqueeze(e9,1),torch.unsqueeze(e10,1),torch.unsqueeze(e11,1),torch.unsqueeze(e12,1),\\\n",
        "                              torch.unsqueeze(e13,1)),1)\n",
        "\n",
        "      _,logits,logit_arou,logit_val, a,b,c,d,e,f,g,h,i,j,k,l,m,inten,logit_target,logit_sarcasm,logit_emotion = self.forward(txt,img)\n",
        "      #self.log('val_acc 1', accuracy_score(lab.detach().cpu().numpy(),np.argmax(logits.detach().cpu().numpy(),axis=-1)))\n",
        "      for n in name:\n",
        "        N.append(n)\n",
        "      append_gt(lab,o_t); append_gt(e1,e1_t); append_gt(e2,e2_t); append_gt(e3,e3_t); append_gt(e4,e4_t); append_gt(e5,e5_t);\\\n",
        "      append_gt(e6,e6_t); append_gt(e7,e7_t); append_gt(e8,e8_t); append_gt(e9,e9_t); append_gt(e10,e10_t); append_gt(e11,e11_t); \\\n",
        "      append_gt(e12,e12_t); append_gt(e13,e13_t); append_gt(intensity,i_t);\n",
        "\n",
        "      append_p(logits,o_p); append_p(a,e1_p); append_p(b,e2_p); append_p(c,e3_p); append_p(d,e4_p); append_p(e,e5_p);\\\n",
        "      append_p(f,e6_p); append_p(g,e7_p); append_p(h,e8_p); append_p(i,e9_p); append_p(j,e10_p); append_p(k,e11_p); \\\n",
        "      append_p(l,e12_p); append_p(m,e13_p); append_p(inten,i_p);\n",
        "\n",
        "      tmp = np.argmax(logits.detach().cpu().numpy(),axis=-1)\n",
        "      loss = self.cross_entropy_loss(logits, lab)\n",
        "      lab = lab.detach().cpu().numpy()\n",
        "      self.log('test_acc', accuracy_score(lab,tmp))\n",
        "      self.log('test f1',f1_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1),average='macro'))\n",
        "      np.save('multitask_logit_emotion.npy',logit_emotion.detach().cpu().numpy())\n",
        "      np.save('multitask_logit_offensive.npy',lab)\n",
        "      np.save('multitask_logit_intensity.npy',inten.detach().cpu().numpy())\n",
        "      #self.log('test confusion matrix',confusion_matrix(lab,tmp))\n",
        "      #print(f'confusion matrix {confusion_matrix(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1))}')\n",
        "      print(f'confusion matrix intensity {confusion_matrix(intensity.detach().cpu().numpy(),np.argmax(inten.detach().cpu().numpy(),axis=-1))}')\n",
        "      print(f'confusion matrix offensive {confusion_matrix(lab,tmp)}')\n",
        "\n",
        "      #self.log('test_roc_auc',roc_auc_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1)))\n",
        "      #self.log('F1',f1_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1)))\n",
        "      #self.log('recall',recall_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1)))\n",
        "      #self.log('precision',precision_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1)))\n",
        "      best_threshold = np.array([0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5])\n",
        "      y_test = torch.nn.Sigmoid()(logit_emotion)\n",
        "      y_test = y_test.detach().cpu().numpy()\n",
        "      y_pred = np.array([[1 if y_test[i][j]>=best_threshold[j] else 0 for j in range(13)] for i in range(len(y_test))])\n",
        "      #print(y_pred)\n",
        "      total_correctly_predicted = len([i for i in range(len(y_test)) if (y_test[i]==y_pred[i]).sum() == 13])\n",
        "      self.log('test_loss', loss)\n",
        "      #print(total_correctly_predicted)\n",
        "      pred_e = y_test\n",
        "      return {'test_loss': loss,\n",
        "              'test_loss_target': F.binary_cross_entropy_with_logits(logit_target.float(), gt_target.float()),\n",
        "              'test_loss_emotion_multilabel': F.binary_cross_entropy_with_logits(logit_emotion.float(), gt_emotion.float()),\n",
        "               'test_acc':f1_score(lab,tmp,average='macro'),\n",
        "              'test_acc e1': accuracy_score(e1.detach().cpu().numpy(),np.argmax(a.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e2': accuracy_score(e2.detach().cpu().numpy(),np.argmax(b.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e3': accuracy_score(e3.detach().cpu().numpy(),np.argmax(c.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e4': accuracy_score(e4.detach().cpu().numpy(),np.argmax(d.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e5': accuracy_score(e5.detach().cpu().numpy(),np.argmax(e.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e6': accuracy_score(e6.detach().cpu().numpy(),np.argmax(f.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e7': accuracy_score(e7.detach().cpu().numpy(),np.argmax(g.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e8': accuracy_score(e8.detach().cpu().numpy(),np.argmax(h.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e9': accuracy_score(e9.detach().cpu().numpy(),np.argmax(i.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e10': accuracy_score(e10.detach().cpu().numpy(),np.argmax(j.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e11': accuracy_score(e11.detach().cpu().numpy(),np.argmax(k.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e12': accuracy_score(e12.detach().cpu().numpy(),np.argmax(l.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e13': accuracy_score(e13.detach().cpu().numpy(),np.argmax(m.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc inten': f1_score(intensity.detach().cpu().numpy(),np.argmax(inten.detach().cpu().numpy(),axis=-1),average='macro'),\n",
        "              'test_acc sarcasm': accuracy_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1)),\n",
        "              'f1 sarcasm': f1_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1),average='macro')}\n",
        "  def test_epoch_end(self, outputs):\n",
        "        # OPTIONAL\n",
        "        outs = []\n",
        "        outs1,outs2,outs3,outs4,outs5,outs6,outs7,outs8,outs9,outs10,outs11,outs12,outs13,outs14 = \\\n",
        "        [],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
        "        outs15 = []\n",
        "        outs16 = []\n",
        "        outs17 = []\n",
        "        outs18 = []\n",
        "        for out in outputs:\n",
        "          outs15.append(out['test_loss_target'])\n",
        "          outs.append(out['test_acc'])\n",
        "          outs1.append(out['test_acc e1'])\n",
        "          outs2.append(out['test_acc e2'])\n",
        "          outs3.append(out['test_acc e3'])\n",
        "          outs4.append(out['test_acc e4'])\n",
        "          outs5.append(out['test_acc e5'])\n",
        "          outs6.append(out['test_acc e6'])\n",
        "          outs7.append(out['test_acc e7'])\n",
        "          outs8.append(out['test_acc e8'])\n",
        "          outs9.append(out['test_acc e9'])\n",
        "          outs10.append(out['test_acc e10'])\n",
        "          outs11.append(out['test_acc e11'])\n",
        "          outs12.append(out['test_acc e12'])\n",
        "          outs13.append(out['test_acc e13'])\n",
        "          outs14.append(out['test_acc inten'])\n",
        "          outs16.append(out['test_acc sarcasm'])\n",
        "          outs17.append(out['test_loss_emotion_multilabel'])\n",
        "          outs18.append(out['f1 sarcasm'])\n",
        "\n",
        "        #print(outs)\n",
        "        self.log('final test f1', sum(outs)/len(outs))\n",
        "        \"\"\"\n",
        "        self.log('test_acc_all e1', sum(outs1)/len(outs1))\n",
        "        self.log('test_acc_all e2', sum(outs2)/len(outs2))\n",
        "        self.log('test_acc_all e3', sum(outs3)/len(outs3))\n",
        "        self.log('test_acc_all e4', sum(outs4)/len(outs4))\n",
        "        self.log('test_acc_all e5', sum(outs5)/len(outs5))\n",
        "        self.log('test_acc_all e6', sum(outs6)/len(outs6))\n",
        "        self.log('test_acc_all e7', sum(outs7)/len(outs7))\n",
        "        self.log('test_acc_all e8', sum(outs8)/len(outs8))\n",
        "        self.log('test_acc_all e9', sum(outs9)/len(outs9))\n",
        "        self.log('test_acc_all e10', sum(outs10)/len(outs10))\n",
        "        self.log('test_acc_all e11', sum(outs11)/len(outs11))\n",
        "        self.log('test_acc_all e12', sum(outs12)/len(outs12))\n",
        "        self.log('test_acc_all e13', sum(outs13)/len(outs13))\n",
        "        self.log('test_acc_all inten', sum(outs14)/len(outs14))\n",
        "        self.log('test_loss_all target', sum(outs15)/len(outs15))\n",
        "        self.log('test_acc_all sarcasm', sum(outs16)/len(outs16))\n",
        "        \"\"\"\n",
        "        self.log('test_loss_all emo', sum(outs17)/len(outs17))\n",
        "        #self.log('test_f1_all sarcasm', sum(outs18)/len(outs18))\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=5e-3)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "class HmDataModule(pl.LightningDataModule):\n",
        "\n",
        "  def setup(self, stage):\n",
        "\n",
        "\n",
        "\n",
        "    self.hm_train = t_p\n",
        "    self.hm_val = v_p\n",
        "    self.hm_test = te_p\n",
        "\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.hm_train, batch_size=64)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.hm_val, batch_size=64)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.hm_test, batch_size=128)\n",
        "\n",
        "data_module = HmDataModule()\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "     monitor='val_acc_all_offn',\n",
        "     dirpath='noemo/ckpts/',\n",
        "     filename='our-ds-ckpt-epoch{epoch:02d}-val_f1_all_offn{val_acc_all_offn:.2f}',\n",
        "     auto_insert_metric_name=False,\n",
        "     save_top_k=1,\n",
        "    mode=\"max\",\n",
        " )\n",
        "all_callbacks = []\n",
        "all_callbacks.append(checkpoint_callback)\n",
        "\"\"\"\n",
        "for i in range(1,14):\n",
        "  tmp_checkpoint_callback = ModelCheckpoint(\n",
        "      monitor='val_acc_all e{}'.format(i),\n",
        "      dirpath='noemo/ckpts/e{}'.format(i),\n",
        "      filename='our-ds-ckpt-best-emo-{}'.format(i),\n",
        "      auto_insert_metric_name=False,\n",
        "      save_top_k=1,\n",
        "      mode=\"max\",\n",
        "  )\n",
        "  all_callbacks.append(tmp_checkpoint_callback)\n",
        "\"\"\"\n",
        "# train\n",
        "from pytorch_lightning import seed_everything\n",
        "seed_everything(seed=123, workers=True)\n",
        "hm_model = Classifier()\n",
        "gpus = 1 if torch.cuda.is_available() else 0\n",
        "trainer = pl.Trainer(gpus=gpus,max_epochs=60,callbacks=all_callbacks)\n",
        "#trainer = pl.Trainer(gpus=gpus,deterministic=True,max_epochs=60,callbacks=all_callbacks)\n",
        "\n",
        "trainer.fit(hm_model, data_module)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wHmxZ3y4yeNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(dataset=te_p, batch_size=1478)\n"
      ],
      "metadata": {
        "id": "vY_6rSW2rA1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls noemo/ckpts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs_2Y8nst3L0",
        "outputId": "45f15fd8-d5bd-41d0-8943-b9b05c2d9722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our-ds-ckpt-epoch36-val_f1_all_offn0.84.ckpt\n",
            "our-ds-ckpt-epoch36-val_f1_all_offn0.84-v1.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_path = '/content/noemo/ckpts/our-ds-ckpt-epoch36-val_f1_all_offn0.84-v1.ckpt' # put ckpt_path according to the path output in the previous cell\n",
        "trainer.test(dataloaders=test_dataloader,ckpt_path=ckpt_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502,
          "referenced_widgets": [
            "4253e3d3a0914818a481840e8d1f576b",
            "d9fdcbb8708b42928a21c65df94037dc",
            "74be07128244435c8e4f1d03beff79f1",
            "cf5da7d42b5e400bb75254cb03b5424f",
            "3ca1232969a54875810a4aa8f84d2c36",
            "1f1dd9e543d547c5a631542308c00f95",
            "3cc7107e0a0f4de9ac62bc9384b14d98",
            "ed2bee8f5d904cde8357cded2a86ed89",
            "905388e2316c47ae8a66944d791c9338",
            "1dbb901316634bec8f9725f985bb6385",
            "19d62ae3d3d94c168ed849fc2a867802"
          ]
        },
        "id": "0F4pk0Zwt7fe",
        "outputId": "aa8fe92e-91bd-4adc-a26b-8d938e3d3345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/datamodule.py:470: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
            "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
            "Restoring states from the checkpoint path at /content/noemo/ckpts/our-ds-ckpt-epoch36-val_f1_all_offn0.84-v1.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from checkpoint at /content/noemo/ckpts/our-ds-ckpt-epoch36-val_f1_all_offn0.84-v1.ckpt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4253e3d3a0914818a481840e8d1f576b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Testing: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion matrix intensity [[391 655  61]\n",
            " [ 35  72   5]\n",
            " [ 77 171  11]]\n",
            "confusion matrix offensive [[978 129]\n",
            " [135 236]]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'final test f1': 0.7611927390098572,\n",
            " 'test f1': 0.1927838772535324,\n",
            " 'test_acc': 0.8213802576065063,\n",
            " 'test_loss': 0.9662419557571411,\n",
            " 'test_loss_all emo': 0.4294007420539856}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/datamodule.py:470: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
            "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'final test f1': 0.7611927390098572,\n",
              "  'test f1': 0.1927838772535324,\n",
              "  'test_acc': 0.8213802576065063,\n",
              "  'test_loss': 0.9662419557571411,\n",
              "  'test_loss_all emo': 0.4294007420539856}]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "e1,e2,e3,e4,e5,e6,e7,e8,e9,e10,e11,e12,e13 = [],[],[],[],[],[],[],[],[],[],[],[],[]\n",
        "for i in test_dataloader:\n",
        "  #print(i)\n",
        "  e1 = i['fear'].numpy()\n",
        "  e2 = i['neglect'].numpy()\n",
        "  e3 = i['irritation'].numpy()\n",
        "  e4 = i['rage'].numpy()\n",
        "  e5 = i['disgust'].numpy()\n",
        "  e6 = i['nervousness'].numpy()\n",
        "  e7 = i['shame'].numpy()\n",
        "  e8 = i['disappointment'].numpy()\n",
        "  e9 = i['envy'].numpy()\n",
        "  e10 = i['suffering'].numpy()\n",
        "  e11 = i['sadness'].numpy()\n",
        "  e12 = i['joy'].numpy()\n",
        "  e13 = i['pride'].numpy()\n",
        "y_test = np.asarray([e1,e2,e3,e4,e5,e6,e7,e8,e9,e10,e11,e12,e13]).T\n"
      ],
      "metadata": {
        "id": "K9xP2etluFdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logit_emo = np.load('multitask_logit_emotion.npy').astype(np.float32)"
      ],
      "metadata": {
        "id": "dZUUc46SxMcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = np.arange(0.01,0.9,0.001)\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "out = logit_emo\n",
        "acc = []\n",
        "accuracies = []\n",
        "best_threshold = np.zeros(out.shape[1])\n",
        "for i in range(out.shape[1]):\n",
        "    y_prob = np.array(out[:,i])\n",
        "    for j in threshold:\n",
        "        y_pred = [1 if prob>=j else 0 for prob in y_prob]\n",
        "        acc.append( matthews_corrcoef(y_test[:,i],y_pred))\n",
        "    acc   = np.array(acc)\n",
        "    index = np.where(acc==acc.max())\n",
        "    accuracies.append(acc.max())\n",
        "    best_threshold[i] = threshold[index[0][0]]\n",
        "    acc = []"
      ],
      "metadata": {
        "id": "Gl_yO2jdux2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred = np.array([[1 if out[i,j]>=best_threshold[j] else 0 for j in range(y_test.shape[1])] for i in range(len(y_test))])"
      ],
      "metadata": {
        "id": "ay8jwkL4uyaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import hamming_loss\n",
        "print(hamming_loss(y_test,y_pred))\n",
        "from sklearn.metrics import f1_score,hamming_loss,precision_score,recall_score\n",
        "print(f1_score(y_test,y_pred,average='micro'))\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test,y_pred,zero_division=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsvgFzGCu1QG",
        "outputId": "6640c803-07e2-4744-b97b-4ca734802bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.13297595503278858\n",
            "0.6004691164972634\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.10      0.17        40\n",
            "           1       0.61      0.60      0.60       513\n",
            "           2       0.50      0.03      0.06        31\n",
            "           3       0.47      0.37      0.41       206\n",
            "           4       0.43      0.18      0.25        73\n",
            "           5       0.46      0.14      0.21        96\n",
            "           6       0.40      0.06      0.10        35\n",
            "           7       0.56      0.48      0.52       621\n",
            "           8       1.00      0.00      0.00        11\n",
            "           9       0.51      0.19      0.27       304\n",
            "          10       0.53      0.31      0.39       426\n",
            "          11       0.91      0.84      0.87      1173\n",
            "          12       0.48      0.30      0.37        89\n",
            "\n",
            "   micro avg       0.69      0.53      0.60      3618\n",
            "   macro avg       0.57      0.28      0.33      3618\n",
            "weighted avg       0.66      0.53      0.57      3618\n",
            " samples avg       0.73      0.57      0.59      3618\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/noemo/ckpts/our-ds-ckpt-epoch35-val_f1_all_offn0.79.ckpt' '/content/drive/MyDrive/memes_our_dataset_hindi/journal_ckpts/'"
      ],
      "metadata": {
        "id": "zOIygfMhxgSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eZGFrjiLPKNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HsVR6BfcPKKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ymVAVo6lPJ9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5n0f9iKNPJ6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wS-WerdRPJ3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ov7n-BwxPJ1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eTlo5ghVPJyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0oxMWeYOPJwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xyfape24PJse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g3QWhaYUPJql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LZNFsoR6PJmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wIATjeajPJlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Zt81mJRPJcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1PgNl6HYPJZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(pl.LightningModule):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.MFB = MFB(640,640,True,256,64,0.1)\n",
        "    self.fin_y_shape = torch.nn.Linear(768,512)\n",
        "    self.fin_old = torch.nn.Linear(2048,2)\n",
        "    self.fin_e = nn.Linear(16 * 768, 64)  # Adjusted input size to match the reshaped output from MFB\n",
        "    self.fin_e1 = torch.nn.Linear(64,2)\n",
        "    self.fin_e2 = torch.nn.Linear(64,2)\n",
        "    self.fin_e3 = torch.nn.Linear(64,2)\n",
        "    self.fin_e4 = torch.nn.Linear(64,2)\n",
        "    self.fin_e5 = torch.nn.Linear(64,2)\n",
        "    self.fin_e6 = torch.nn.Linear(64,2)\n",
        "    self.fin_e7 = torch.nn.Linear(64,2)\n",
        "    self.fin_e8 = torch.nn.Linear(64,2)\n",
        "    self.fin_e9 = torch.nn.Linear(64,2)\n",
        "    self.fin_inten = torch.nn.Linear(64,3)\n",
        "\n",
        "\n",
        "  def forward(self, x,y):\n",
        "\n",
        "      x_,y_ = x,y\n",
        "      x = x.float()\n",
        "      y = y.float()\n",
        "      z_ = self.MFB(torch.unsqueeze(y,axis=1),torch.unsqueeze(x,axis=1))\n",
        "      z = z_\n",
        "      c = self.fin(torch.squeeze(z,dim=1))\n",
        "      c_inten = self.fin_inten(torch.squeeze(z,dim=1))\n",
        "      c_v = self.fin_v(torch.squeeze(z,dim=1))\n",
        "      c_a = self.fin_a(torch.squeeze(z,dim=1))\n",
        "      c_e1 = self.fin_e1(torch.squeeze(z,dim=1))\n",
        "      c_e2 = self.fin_e2(torch.squeeze(z,dim=1))\n",
        "      c_e3 = self.fin_e3(torch.squeeze(z,dim=1))\n",
        "      c_e4 = self.fin_e4(torch.squeeze(z,dim=1))\n",
        "      c_e5 = self.fin_e5(torch.squeeze(z,dim=1))\n",
        "      c_e6 = self.fin_e6(torch.squeeze(z,dim=1))\n",
        "      c_e7 = self.fin_e7(torch.squeeze(z,dim=1))\n",
        "      c_e8 = self.fin_e8(torch.squeeze(z,dim=1))\n",
        "      c_e9 = self.fin_e9(torch.squeeze(z,dim=1))\n",
        "      c_e10 = self.fin_e10(torch.squeeze(z,dim=1))\n",
        "      c_e11 = self.fin_e11(torch.squeeze(z,dim=1))\n",
        "      c_e12 = self.fin_e12(torch.squeeze(z,dim=1))\n",
        "      c_e13 = self.fin_e13(torch.squeeze(z,dim=1))\n",
        "      c_sarcasm = self.fin_sarcasm(torch.squeeze(z,dim=1))\n",
        "      # probability distribution over labels\n",
        "      c = torch.log_softmax(c, dim=1)\n",
        "      c_inten = torch.log_softmax(c_inten, dim=1)\n",
        "      c_a = torch.log_softmax(c_a, dim=1)\n",
        "      c_v = torch.log_softmax(c_v, dim=1)\n",
        "      c_e1 = torch.log_softmax(c_e1, dim=1)\n",
        "      c_sarcasm = torch.log_softmax(c_sarcasm, dim=1)\n",
        "      c_e2 = torch.log_softmax(c_e2, dim=1)\n",
        "      c_e3 = torch.log_softmax(c_e3, dim=1)\n",
        "      c_e4 = torch.log_softmax(c_e4, dim=1)\n",
        "\n",
        "      c_e5 = torch.log_softmax(c_e5, dim=1)\n",
        "      c_e6 = torch.log_softmax(c_e6, dim=1)\n",
        "      c_e7 = torch.log_softmax(c_e7, dim=1)\n",
        "\n",
        "      c_e8 = torch.log_softmax(c_e8, dim=1)\n",
        "      c_e9 = torch.log_softmax(c_e9, dim=1)\n",
        "      c_e10 = torch.log_softmax(c_e10, dim=1)\n",
        "\n",
        "      c_e11 = torch.log_softmax(c_e11, dim=1)\n",
        "      c_e12 = torch.log_softmax(c_e12, dim=1)\n",
        "      c_e13 = torch.log_softmax(c_e13, dim=1)\n",
        "      c_target = self.fin_target_ident(torch.squeeze(z,dim=1))\n",
        "      c_emotion = self.fin_emotion_mult(torch.squeeze(z,dim=1))\n",
        "      return z,c,c_a,c_v,c_e1,c_e2,c_e3,c_e4,c_e5,c_e6,c_e7,c_e8,c_e9,c_e10,c_e11,c_e12, c_e13, c_inten, c_target, c_sarcasm, c_emotion\n",
        "\n",
        "  def cross_entropy_loss(self, logits, labels):\n",
        "    return F.nll_loss(logits, labels)\n",
        "\n",
        "\n",
        "  def training_step(self, train_batch, batch_idx):\n",
        "      _,lab,txt,img,val,arou,e1,e2,e3,e4,e5,e6,e7,e8,e9,e10,e11,e12,e13,sarcasm,_,intensity,t1,t2,t3,t4,t5,t6,t7 = train_batch\n",
        "\n",
        "      lab = train_batch[lab]\n",
        "      #print(lab)\n",
        "      txt = train_batch[txt]\n",
        "      #print(txt)\n",
        "\n",
        "      img = train_batch[img]\n",
        "      val = train_batch[val]\n",
        "      arou = train_batch[arou]\n",
        "      #print(a)\n",
        "      e1 = train_batch[e1]\n",
        "      e2 = train_batch[e2]\n",
        "      e3 = train_batch[e3]\n",
        "      e4 = train_batch[e4]\n",
        "      e5 = train_batch[e5]\n",
        "      e6 = train_batch[e6]\n",
        "      e7 = train_batch[e7]\n",
        "      e8 = train_batch[e8]\n",
        "      e9 = train_batch[e9]\n",
        "      e10 = train_batch[e10]\n",
        "      e11 = train_batch[e11]\n",
        "      e12 = train_batch[e12]\n",
        "      e13 = train_batch[e13]\n",
        "      intensity = train_batch[intensity]\n",
        "      sarcasm = train_batch[sarcasm]\n",
        "      t1,t2,t3,t4,t5,t6,t7 = torch.unsqueeze(train_batch[t1],1),torch.unsqueeze(train_batch[t2],1),\\\n",
        "      torch.unsqueeze(train_batch[t3],1),torch.unsqueeze(train_batch[t4],1),torch.unsqueeze(train_batch[t5],1),\\\n",
        "      torch.unsqueeze(train_batch[t6],1),torch.unsqueeze(train_batch[t7],1)\n",
        "      gt_target = torch.cat((t1,t2,t3,t4,t5,t6,t7),1) #ground truth target\n",
        "\n",
        "      gt_emotion = torch.cat((torch.unsqueeze(e1,1),torch.unsqueeze(e2,1),torch.unsqueeze(e3,1),torch.unsqueeze(e4,1),torch.unsqueeze(e5,1),torch.unsqueeze(e6,1),\\\n",
        "                              torch.unsqueeze(e7,1),torch.unsqueeze(e8,1),torch.unsqueeze(e9,1),torch.unsqueeze(e10,1),torch.unsqueeze(e11,1),torch.unsqueeze(e12,1),\\\n",
        "                              torch.unsqueeze(e13,1)),1)\n",
        "\n",
        "      z,logit_offen,logit_arou,logit_val, a,b,c,d,e,f,g,h,i,j,k,l,m,,logitinten_target,logit_sarcasm,logit_emotion = self.forward(txt,img) # logit_target is logits of target\n",
        "\n",
        "      loss1 = self.cross_entropy_loss(logit_offen, lab)\n",
        "      #loss2 = self.cross_entropy_loss(logit_arou, arou)\n",
        "      #loss3 = self.cross_entropy_loss(logit_val, val)\n",
        "      loss4 = self.cross_entropy_loss(a, e1)\n",
        "      loss5 = self.cross_entropy_loss(b, e2)\n",
        "      loss6 = self.cross_entropy_loss(c, e3)\n",
        "      loss7 = self.cross_entropy_loss(d, e4)\n",
        "      loss8 = self.cross_entropy_loss(e, e5)\n",
        "      loss9 = self.cross_entropy_loss(f, e6)\n",
        "      loss10 = self.cross_entropy_loss(g, e7)\n",
        "      loss11 = self.cross_entropy_loss(h, e8)\n",
        "      loss12 = self.cross_entropy_loss(i, e9)\n",
        "      loss13 = self.cross_entropy_loss(j, e10)\n",
        "      loss14 = self.cross_entropy_loss(k, e11)\n",
        "      loss15 = self.cross_entropy_loss(l, e12)\n",
        "      loss16 = self.cross_entropy_loss(m, e13)\n",
        "      loss17 = self.cross_entropy_loss(inten, intensity)\n",
        "\n",
        "      loss18 = F.binary_cross_entropy_with_logits(logit_target.float(), gt_target.float())\n",
        "      loss_emo_mult = F.binary_cross_entropy_with_logits(logit_emotion.float(), gt_emotion.float())\n",
        "      loss_sarcasm = self.cross_entropy_loss(logit_sarcasm, sarcasm)\n",
        "\n",
        "      #loss = loss1+loss_emo_mult+loss17\n",
        "      loss=loss1+loss_emo_mult\n",
        "      self.log('train_loss', loss)\n",
        "\n",
        "      return loss\n",
        "\n",
        "\n",
        "  def validation_step(self, val_batch, batch_idx):\n",
        "      _,lab,txt,img,val,arou,e1,e2,e3,e4,e5,e6,e7,e8,e9,e10,e11,e12,e13,sarcasm,_,intensity,t1,t2,t3,t4,t5,t6,t7 = val_batch\n",
        "      #print(val_batch)\n",
        "      lab = val_batch[lab]\n",
        "      txt = val_batch[txt]\n",
        "      img = val_batch[img]\n",
        "      val = val_batch[val]\n",
        "      arou = val_batch[arou]\n",
        "      e1 = val_batch[e1]\n",
        "      e2 = val_batch[e2]\n",
        "      e3 = val_batch[e3]\n",
        "      e4 = val_batch[e4]\n",
        "      e5 = val_batch[e5]\n",
        "      e6 = val_batch[e6]\n",
        "      e7 = val_batch[e7]\n",
        "      e8 = val_batch[e8]\n",
        "      e9 = val_batch[e9]\n",
        "      e10 = val_batch[e10]\n",
        "      e11 = val_batch[e11]\n",
        "      e12 = val_batch[e12]\n",
        "      e13 = val_batch[e13]\n",
        "      sarcasm = val_batch[sarcasm]\n",
        "      intensity = val_batch[intensity]\n",
        "      t1,t2,t3,t4,t5,t6,t7 = torch.unsqueeze(val_batch[t1],1),torch.unsqueeze(val_batch[t2],1),\\\n",
        "      torch.unsqueeze(val_batch[t3],1),torch.unsqueeze(val_batch[t4],1),torch.unsqueeze(val_batch[t5],1),\\\n",
        "      torch.unsqueeze(val_batch[t6],1),torch.unsqueeze(val_batch[t7],1)\n",
        "      #print(t1.size())\n",
        "      gt_target = torch.cat((t1,t2,t3,t4,t5,t6,t7),1) #ground truth target\n",
        "      gt_emotion = torch.cat((torch.unsqueeze(e1,1),torch.unsqueeze(e2,1),torch.unsqueeze(e3,1),torch.unsqueeze(e4,1),torch.unsqueeze(e5,1),torch.unsqueeze(e6,1),\\\n",
        "                              torch.unsqueeze(e7,1),torch.unsqueeze(e8,1),torch.unsqueeze(e9,1),torch.unsqueeze(e10,1),torch.unsqueeze(e11,1),torch.unsqueeze(e12,1),\\\n",
        "                              torch.unsqueeze(e13,1)),1)\n",
        "\n",
        "      _,logits,logit_arou,logit_val, a,b,c,d,e,f,g,h,i,j,k,l,m,inten,logit_target,logit_sarcasm,logit_emotion = self.forward(txt,img)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      tmp = np.argmax(logits.detach().cpu().numpy(),axis=-1)\n",
        "      loss = self.cross_entropy_loss(logits, lab)\n",
        "      lab = lab.detach().cpu().numpy()\n",
        "      self.log('val_acc', f1_score(lab,tmp,average='macro'))\n",
        "      #self.log('val_roc_auc',roc_auc_score(lab,tmp))\n",
        "      self.log('val_loss', loss)\n",
        "      tqdm_dict = {'val_acc': accuracy_score(lab,tmp)}\n",
        "      #print('Val acc {}'.format(accuracy_score(lab,tmp)))\n",
        "      return {\n",
        "                'progress_bar': tqdm_dict,\n",
        "              'val_loss_target': F.binary_cross_entropy_with_logits(logit_target.float(), gt_target.float()),\n",
        "              'val_loss_emotion_multilabel': F.binary_cross_entropy_with_logits(logit_emotion.float(), gt_emotion.float()),\n",
        "              'val_acc e1': accuracy_score(e1.detach().cpu().numpy(),np.argmax(a.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e2': accuracy_score(e2.detach().cpu().numpy(),np.argmax(b.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e3': accuracy_score(e3.detach().cpu().numpy(),np.argmax(c.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e4': accuracy_score(e4.detach().cpu().numpy(),np.argmax(d.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e5': accuracy_score(e5.detach().cpu().numpy(),np.argmax(e.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e6': accuracy_score(e6.detach().cpu().numpy(),np.argmax(f.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e7': accuracy_score(e7.detach().cpu().numpy(),np.argmax(g.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e8': accuracy_score(e8.detach().cpu().numpy(),np.argmax(h.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e9': accuracy_score(e9.detach().cpu().numpy(),np.argmax(i.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e10': accuracy_score(e10.detach().cpu().numpy(),np.argmax(j.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e11': accuracy_score(e11.detach().cpu().numpy(),np.argmax(k.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e12': accuracy_score(e12.detach().cpu().numpy(),np.argmax(l.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc e13': accuracy_score(e13.detach().cpu().numpy(),np.argmax(m.detach().cpu().numpy(),axis=-1)),\n",
        "      'val_acc intensity': f1_score(intensity.detach().cpu().numpy(),np.argmax(inten.detach().cpu().numpy(),axis=-1),average='macro'),\n",
        "       'val_acc sarcasm': accuracy_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1)),\n",
        "       'f1 sarcasm': f1_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1),average='macro')\n",
        "      }\n",
        "\n",
        "  def validation_epoch_end(self, validation_step_outputs):\n",
        "    outs = []\n",
        "    outs1,outs2,outs3,outs4,outs5,outs6,outs7,outs8,outs9,outs10,outs11,outs12,outs13,outs14,outs16,outs17 = \\\n",
        "    [],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
        "    outs15 = []\n",
        "    outs18 = []\n",
        "    for out in validation_step_outputs:\n",
        "      outs.append(out['progress_bar']['val_acc'])\n",
        "      outs1.append(out['val_acc e1'])\n",
        "      outs2.append(out['val_acc e2'])\n",
        "      outs3.append(out['val_acc e3'])\n",
        "      outs4.append(out['val_acc e4'])\n",
        "      outs5.append(out['val_acc e5'])\n",
        "      outs6.append(out['val_acc e6'])\n",
        "      outs7.append(out['val_acc e7'])\n",
        "      outs8.append(out['val_acc e8'])\n",
        "      outs9.append(out['val_acc e9'])\n",
        "      outs10.append(out['val_acc e10'])\n",
        "      outs11.append(out['val_acc e11'])\n",
        "      outs12.append(out['val_acc e12'])\n",
        "      outs13.append(out['val_acc e13'])\n",
        "      outs14.append(out['val_acc intensity'])\n",
        "      outs15.append(out['val_loss_target'])\n",
        "      outs16.append(out['val_loss_emotion_multilabel'])\n",
        "      outs17.append(out['val_acc sarcasm'])\n",
        "      outs18.append(out['f1 sarcasm'])\n",
        "    self.log('val_acc_all_offn', sum(outs)/len(outs))\n",
        "    self.log('val_loss_target', sum(outs15)/len(outs15))\n",
        "    self.log('val_acc_all e1', sum(outs1)/len(outs1))\n",
        "    self.log('val_acc_all e2', sum(outs2)/len(outs2))\n",
        "    self.log('val_acc_all e3', sum(outs3)/len(outs3))\n",
        "    self.log('val_acc_all e4', sum(outs4)/len(outs4))\n",
        "    self.log('val_acc_all e5', sum(outs5)/len(outs5))\n",
        "    self.log('val_acc_all e6', sum(outs6)/len(outs6))\n",
        "    self.log('val_acc_all e7', sum(outs7)/len(outs7))\n",
        "    self.log('val_acc_all e8', sum(outs8)/len(outs8))\n",
        "    self.log('val_acc_all e9', sum(outs9)/len(outs9))\n",
        "    self.log('val_acc_all e10', sum(outs10)/len(outs10))\n",
        "    self.log('val_acc_all e11', sum(outs11)/len(outs11))\n",
        "    self.log('val_acc_all e12', sum(outs12)/len(outs12))\n",
        "    self.log('val_acc_all e13', sum(outs13)/len(outs13))\n",
        "    self.log('val_acc_all inten', sum(outs14)/len(outs14))\n",
        "    self.log('val_loss_all emo', sum(outs16)/len(outs16))\n",
        "    self.log('val_acc_all sarcasm', sum(outs17)/len(outs17))\n",
        "    self.log('val_f1_all sarcasm', sum(outs18)/len(outs18))\n",
        "\n",
        "    print(f'***offensive f1 at epoch end {sum(outs)/len(outs)}****')\n",
        "    #print(f'***val acc inten at epoch end {sum(outs14)/len(outs14)}****')\n",
        "    print(f'***val loss emotion at epoch end {sum(outs16)/len(outs16)}****')\n",
        "    #print(f'***val acc sarcasm at epoch end {sum(outs17)/len(outs17)}****')\n",
        "    #print(f'***val f1 sarcasm at epoch end {sum(outs18)/len(outs18)}****')\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "      name,lab,txt,img,val,arou,e1,e2,e3,e4,e5,e6,e7,e8,e9,e10,e11,e12,e13,sarcasm,_,intensity,t1,t2,t3,t4,t5,t6,t7 = batch\n",
        "      name = batch[name]\n",
        "      lab = batch[lab]\n",
        "      txt = batch[txt]\n",
        "      img = batch[img]\n",
        "      e1 = batch[e1]\n",
        "      e2 = batch[e2]\n",
        "      e3 = batch[e3]\n",
        "      e4 = batch[e4]\n",
        "      e5 = batch[e5]\n",
        "      e6 = batch[e6]\n",
        "      e7 = batch[e7]\n",
        "      e8 = batch[e8]\n",
        "      e9 = batch[e9]\n",
        "      e10 = batch[e10]\n",
        "      e11 = batch[e11]\n",
        "      e12 = batch[e12]\n",
        "      e13 = batch[e13]\n",
        "      intensity = batch[intensity]\n",
        "      sarcasm = batch[sarcasm]\n",
        "      t1,t2,t3,t4,t5,t6,t7 = torch.unsqueeze(batch[t1],1),torch.unsqueeze(batch[t2],1),\\\n",
        "      torch.unsqueeze(batch[t3],1),torch.unsqueeze(batch[t4],1),torch.unsqueeze(batch[t5],1),\\\n",
        "      torch.unsqueeze(batch[t6],1),torch.unsqueeze(batch[t7],1)\n",
        "      gt_target = torch.cat((t1,t2,t3,t4,t5,t6,t7),1)\n",
        "      gt_emotion = torch.cat((torch.unsqueeze(e1,1),torch.unsqueeze(e2,1),torch.unsqueeze(e3,1),torch.unsqueeze(e4,1),torch.unsqueeze(e5,1),torch.unsqueeze(e6,1),\\\n",
        "                              torch.unsqueeze(e7,1),torch.unsqueeze(e8,1),torch.unsqueeze(e9,1),torch.unsqueeze(e10,1),torch.unsqueeze(e11,1),torch.unsqueeze(e12,1),\\\n",
        "                              torch.unsqueeze(e13,1)),1)\n",
        "\n",
        "      _,logits,logit_arou,logit_val, a,b,c,d,e,f,g,h,i,j,k,l,m,inten,logit_target,logit_sarcasm,logit_emotion = self.forward(txt,img)\n",
        "      #self.log('val_acc 1', accuracy_score(lab.detach().cpu().numpy(),np.argmax(logits.detach().cpu().numpy(),axis=-1)))\n",
        "      for n in name:\n",
        "        N.append(n)\n",
        "      append_gt(lab,o_t); append_gt(e1,e1_t); append_gt(e2,e2_t); append_gt(e3,e3_t); append_gt(e4,e4_t); append_gt(e5,e5_t);\\\n",
        "      append_gt(e6,e6_t); append_gt(e7,e7_t); append_gt(e8,e8_t); append_gt(e9,e9_t); append_gt(e10,e10_t); append_gt(e11,e11_t); \\\n",
        "      append_gt(e12,e12_t); append_gt(e13,e13_t); append_gt(intensity,i_t);\n",
        "\n",
        "      append_p(logits,o_p); append_p(a,e1_p); append_p(b,e2_p); append_p(c,e3_p); append_p(d,e4_p); append_p(e,e5_p);\\\n",
        "      append_p(f,e6_p); append_p(g,e7_p); append_p(h,e8_p); append_p(i,e9_p); append_p(j,e10_p); append_p(k,e11_p); \\\n",
        "      append_p(l,e12_p); append_p(m,e13_p); append_p(inten,i_p);\n",
        "\n",
        "      tmp = np.argmax(logits.detach().cpu().numpy(),axis=-1)\n",
        "      loss = self.cross_entropy_loss(logits, lab)\n",
        "      lab = lab.detach().cpu().numpy()\n",
        "      self.log('test_acc', accuracy_score(lab,tmp))\n",
        "      self.log('test f1',f1_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1),average='macro'))\n",
        "      np.save('multitask_logit_emotion.npy',logit_emotion.detach().cpu().numpy())\n",
        "      np.save('multitask_logit_offensive.npy',lab)\n",
        "      np.save('multitask_logit_intensity.npy',inten.detach().cpu().numpy())\n",
        "      #self.log('test confusion matrix',confusion_matrix(lab,tmp))\n",
        "      #print(f'confusion matrix {confusion_matrix(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1))}')\n",
        "      print(f'confusion matrix intensity {confusion_matrix(intensity.detach().cpu().numpy(),np.argmax(inten.detach().cpu().numpy(),axis=-1))}')\n",
        "      print(f'confusion matrix offensive {confusion_matrix(lab,tmp)}')\n",
        "\n",
        "      #self.log('test_roc_auc',roc_auc_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1)))\n",
        "      #self.log('F1',f1_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1)))\n",
        "      #self.log('recall',recall_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1)))\n",
        "      #self.log('precision',precision_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1)))\n",
        "      best_threshold = np.array([0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5])\n",
        "      y_test = torch.nn.Sigmoid()(logit_emotion)\n",
        "      y_test = y_test.detach().cpu().numpy()\n",
        "      y_pred = np.array([[1 if y_test[i][j]>=best_threshold[j] else 0 for j in range(13)] for i in range(len(y_test))])\n",
        "      #print(y_pred)\n",
        "      total_correctly_predicted = len([i for i in range(len(y_test)) if (y_test[i]==y_pred[i]).sum() == 13])\n",
        "      self.log('test_loss', loss)\n",
        "      #print(total_correctly_predicted)\n",
        "      pred_e = y_test\n",
        "      return {'test_loss': loss,\n",
        "              'test_loss_target': F.binary_cross_entropy_with_logits(logit_target.float(), gt_target.float()),\n",
        "              'test_loss_emotion_multilabel': F.binary_cross_entropy_with_logits(logit_emotion.float(), gt_emotion.float()),\n",
        "               'test_acc':f1_score(lab,tmp,average='macro'),\n",
        "              'test_acc e1': accuracy_score(e1.detach().cpu().numpy(),np.argmax(a.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e2': accuracy_score(e2.detach().cpu().numpy(),np.argmax(b.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e3': accuracy_score(e3.detach().cpu().numpy(),np.argmax(c.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e4': accuracy_score(e4.detach().cpu().numpy(),np.argmax(d.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e5': accuracy_score(e5.detach().cpu().numpy(),np.argmax(e.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e6': accuracy_score(e6.detach().cpu().numpy(),np.argmax(f.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e7': accuracy_score(e7.detach().cpu().numpy(),np.argmax(g.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e8': accuracy_score(e8.detach().cpu().numpy(),np.argmax(h.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e9': accuracy_score(e9.detach().cpu().numpy(),np.argmax(i.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e10': accuracy_score(e10.detach().cpu().numpy(),np.argmax(j.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e11': accuracy_score(e11.detach().cpu().numpy(),np.argmax(k.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e12': accuracy_score(e12.detach().cpu().numpy(),np.argmax(l.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc e13': accuracy_score(e13.detach().cpu().numpy(),np.argmax(m.detach().cpu().numpy(),axis=-1)),\n",
        "              'test_acc inten': f1_score(intensity.detach().cpu().numpy(),np.argmax(inten.detach().cpu().numpy(),axis=-1),average='macro'),\n",
        "              'test_acc sarcasm': accuracy_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1)),\n",
        "              'f1 sarcasm': f1_score(sarcasm.detach().cpu().numpy(),np.argmax(logit_sarcasm.detach().cpu().numpy(),axis=-1),average='macro')}\n",
        "  def test_epoch_end(self, outputs):\n",
        "        # OPTIONAL\n",
        "        outs = []\n",
        "        outs1,outs2,outs3,outs4,outs5,outs6,outs7,outs8,outs9,outs10,outs11,outs12,outs13,outs14 = \\\n",
        "        [],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
        "        outs15 = []\n",
        "        outs16 = []\n",
        "        outs17 = []\n",
        "        outs18 = []\n",
        "        for out in outputs:\n",
        "          outs15.append(out['test_loss_target'])\n",
        "          outs.append(out['test_acc'])\n",
        "          outs1.append(out['test_acc e1'])\n",
        "          outs2.append(out['test_acc e2'])\n",
        "          outs3.append(out['test_acc e3'])\n",
        "          outs4.append(out['test_acc e4'])\n",
        "          outs5.append(out['test_acc e5'])\n",
        "          outs6.append(out['test_acc e6'])\n",
        "          outs7.append(out['test_acc e7'])\n",
        "          outs8.append(out['test_acc e8'])\n",
        "          outs9.append(out['test_acc e9'])\n",
        "          outs10.append(out['test_acc e10'])\n",
        "          outs11.append(out['test_acc e11'])\n",
        "          outs12.append(out['test_acc e12'])\n",
        "          outs13.append(out['test_acc e13'])\n",
        "          outs14.append(out['test_acc inten'])\n",
        "          outs16.append(out['test_acc sarcasm'])\n",
        "          outs17.append(out['test_loss_emotion_multilabel'])\n",
        "          outs18.append(out['f1 sarcasm'])\n",
        "\n",
        "        #print(outs)\n",
        "        self.log('final test f1', sum(outs)/len(outs))\n",
        "        \"\"\"\n",
        "        self.log('test_acc_all e1', sum(outs1)/len(outs1))\n",
        "        self.log('test_acc_all e2', sum(outs2)/len(outs2))\n",
        "        self.log('test_acc_all e3', sum(outs3)/len(outs3))\n",
        "        self.log('test_acc_all e4', sum(outs4)/len(outs4))\n",
        "        self.log('test_acc_all e5', sum(outs5)/len(outs5))\n",
        "        self.log('test_acc_all e6', sum(outs6)/len(outs6))\n",
        "        self.log('test_acc_all e7', sum(outs7)/len(outs7))\n",
        "        self.log('test_acc_all e8', sum(outs8)/len(outs8))\n",
        "        self.log('test_acc_all e9', sum(outs9)/len(outs9))\n",
        "        self.log('test_acc_all e10', sum(outs10)/len(outs10))\n",
        "        self.log('test_acc_all e11', sum(outs11)/len(outs11))\n",
        "        self.log('test_acc_all e12', sum(outs12)/len(outs12))\n",
        "        self.log('test_acc_all e13', sum(outs13)/len(outs13))\n",
        "        self.log('test_acc_all inten', sum(outs14)/len(outs14))\n",
        "        self.log('test_loss_all target', sum(outs15)/len(outs15))\n",
        "        self.log('test_acc_all sarcasm', sum(outs16)/len(outs16))\n",
        "        \"\"\"\n",
        "        self.log('test_loss_all emo', sum(outs17)/len(outs17))\n",
        "        #self.log('test_f1_all sarcasm', sum(outs18)/len(outs18))\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=5e-3)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "class HmDataModule(pl.LightningDataModule):\n",
        "\n",
        "  def setup(self, stage):\n",
        "\n",
        "\n",
        "\n",
        "    self.hm_train = t_p\n",
        "    self.hm_val = v_p\n",
        "    self.hm_test = te_p\n",
        "\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.hm_train, batch_size=64)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.hm_val, batch_size=64)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.hm_test, batch_size=128)\n",
        "\n",
        "data_module = HmDataModule()\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "     monitor='val_acc_all_offn',\n",
        "     dirpath='noemo/ckpts/',\n",
        "     filename='our-ds-ckpt-epoch{epoch:02d}-val_f1_all_offn{val_acc_all_offn:.2f}',\n",
        "     auto_insert_metric_name=False,\n",
        "     save_top_k=1,\n",
        "    mode=\"max\",\n",
        " )\n",
        "all_callbacks = []\n",
        "all_callbacks.append(checkpoint_callback)\n",
        "\"\"\"\n",
        "for i in range(1,14):\n",
        "  tmp_checkpoint_callback = ModelCheckpoint(\n",
        "      monitor='val_acc_all e{}'.format(i),\n",
        "      dirpath='noemo/ckpts/e{}'.format(i),\n",
        "      filename='our-ds-ckpt-best-emo-{}'.format(i),\n",
        "      auto_insert_metric_name=False,\n",
        "      save_top_k=1,\n",
        "      mode=\"max\",\n",
        "  )\n",
        "  all_callbacks.append(tmp_checkpoint_callback)\n",
        "\"\"\"\n",
        "# train\n",
        "from pytorch_lightning import seed_everything\n",
        "seed_everything(seed=123, workers=True)\n",
        "hm_model = Classifier()\n",
        "gpus = 1 if torch.cuda.is_available() else 0\n",
        "trainer = pl.Trainer(gpus=gpus,max_epochs=60,callbacks=all_callbacks)\n",
        "#trainer = pl.Trainer(gpus=gpus,deterministic=True,max_epochs=60,callbacks=all_callbacks)\n",
        "\n",
        "trainer.fit(hm_model, data_module)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m9YH1AsrdG8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s6_WXsAxSUuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NyE6b0JhSUpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7zfKTs-ASUjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sh55FXOmSUcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wAsEz0-6SUVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_e = 0\n",
        "import torch\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score,precision_score\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import os\n",
        "\n",
        "class Classifier(pl.LightningModule):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.MFB = MFB(640,640,True,256,64,0.1)\n",
        "    self.loss_fn_emotion=torch.nn.KLDivLoss(reduction='batchmean',log_target=True)\n",
        "    self.encode_text = torch.nn.Linear(1280,64)\n",
        "    self.fin = torch.nn.Linear(64,2)\n",
        "    self.fin_v = torch.nn.Linear(64,4)\n",
        "    self.fin_a = torch.nn.Linear(64,4)\n",
        "    self.fin_inten = torch.nn.Linear(64,3)\n",
        "    self.fin_target_ident = torch.nn.Linear(64,7)\n",
        "    self.fin_emotion_mult = torch.nn.Linear(64,13)\n",
        "\n",
        "\n",
        "  def forward(self, x,y):\n",
        "\n",
        "      x_,y_ = x,y\n",
        "\n",
        "      z_ = self.MFB(torch.unsqueeze(y,axis=1),torch.unsqueeze(x,axis=1))\n",
        "      z = z_\n",
        "      c = self.fin(torch.squeeze(z,dim=1))\n",
        "      c_inten = self.fin_inten(torch.squeeze(z,dim=1))\n",
        "\n",
        "      c = torch.log_softmax(c, dim=1)\n",
        "      c_inten = torch.log_softmax(c_inten, dim=1)\n",
        "\n",
        "      c_target = self.fin_target_ident(torch.squeeze(z,dim=1))\n",
        "      c_emotion = self.fin_emotion_mult(torch.squeeze(z,dim=1))\n",
        "      return z,c,c_inten, c_target\n",
        "\n",
        "  def cross_entropy_loss(self, logits, labels):\n",
        "    return F.nll_loss(logits, labels)\n",
        "\n",
        "\n",
        "  def training_step(self, train_batch, batch_idx):\n",
        "      _,lab,txt,img,intensity,t1,t2,t3,t4,t5,t6,t7 = train_batch\n",
        "      lab = train_batch[lab]\n",
        "      txt = train_batch[txt]\n",
        "      img = train_batch[img]\n",
        "\n",
        "      intensity = train_batch[intensity]\n",
        "      sarcasm = train_batch[sarcasm]\n",
        "      t1,t2,t3,t4,t5,t6,t7 = torch.unsqueeze(train_batch[t1],1),torch.unsqueeze(train_batch[t2],1),\\\n",
        "      torch.unsqueeze(train_batch[t3],1),torch.unsqueeze(train_batch[t4],1),torch.unsqueeze(train_batch[t5],1),\\\n",
        "      torch.unsqueeze(train_batch[t6],1),torch.unsqueeze(train_batch[t7],1)\n",
        "      gt_target = torch.cat((t1,t2,t3,t4,t5,t6,t7),1) #ground truth target\n",
        "\n",
        "      z,logit_offen,logitinten_target, = self.forward(txt,img) # logit_target is logits of target\n",
        "\n",
        "      loss1 = self.cross_entropy_loss(logit_offen, lab)\n",
        "\n",
        "      loss17 = self.cross_entropy_loss(inten, intensity)\n",
        "\n",
        "      loss18 = F.binary_cross_entropy_with_logits(logit_target.float(), gt_target.float())\n",
        "\n",
        "      #loss = loss1+loss_emo_mult+loss17\n",
        "      loss=loss1+loss_emo_mult\n",
        "      self.log('train_loss', loss)\n",
        "\n",
        "      return loss\n",
        "\n",
        "\n",
        "  def validation_step(self, val_batch, batch_idx):\n",
        "      _,lab,txt,img,intensity,t1,t2,t3,t4,t5,t6,t7 = val_batch\n",
        "      #print(val_batch)\n",
        "      lab = val_batch[lab]\n",
        "      txt = val_batch[txt]\n",
        "      img = val_batch[img]\n",
        "      intensity = val_batch[intensity]\n",
        "      t1,t2,t3,t4,t5,t6,t7 = torch.unsqueeze(val_batch[t1],1),torch.unsqueeze(val_batch[t2],1),\\\n",
        "      torch.unsqueeze(val_batch[t3],1),torch.unsqueeze(val_batch[t4],1),torch.unsqueeze(val_batch[t5],1),\\\n",
        "      torch.unsqueeze(val_batch[t6],1),torch.unsqueeze(val_batch[t7],1)\n",
        "      #print(t1.size())\n",
        "      gt_target = torch.cat((t1,t2,t3,t4,t5,t6,t7),1) #ground truth target\n",
        "\n",
        "      _,logits,logit_arou,logit_val,inten,logit_target,logit_sarcasm,logit_emotion = self.forward(txt,img)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      tmp = np.argmax(logits.detach().cpu().numpy(),axis=-1)\n",
        "      loss = self.cross_entropy_loss(logits, lab)\n",
        "      lab = lab.detach().cpu().numpy()\n",
        "      self.log('val_acc', f1_score(lab,tmp,average='macro'))\n",
        "      self.log('val_loss', loss)\n",
        "      tqdm_dict = {'val_acc': accuracy_score(lab,tmp)}\n",
        "\n",
        "      return {\n",
        "                'progress_bar': tqdm_dict,\n",
        "              'val_loss_target': F.binary_cross_entropy_with_logits(logit_target.float(), gt_target.float()),\n",
        "\n",
        "      'val_acc intensity': f1_score(intensity.detach().cpu().numpy(),np.argmax(inten.detach().cpu().numpy(),axis=-1),average='macro'),\n",
        "       }\n",
        "\n",
        "  def validation_epoch_end(self, validation_step_outputs):\n",
        "    outs = []\n",
        "    outs1,outs2,outs3,outs4,outs5,outs6,outs7,outs8,outs9,outs10,outs11,outs12,outs13,outs14,outs16,outs17 = \\\n",
        "    [],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
        "    outs15 = []\n",
        "    outs18 = []\n",
        "    for out in validation_step_outputs:\n",
        "      outs.append(out['progress_bar']['val_acc'])\n",
        "\n",
        "      outs14.append(out['val_acc intensity'])\n",
        "      outs15.append(out['val_loss_target'])\n",
        "\n",
        "      outs18.append(out['f1 sarcasm'])\n",
        "    self.log('val_acc_all_offn', sum(outs)/len(outs))\n",
        "    self.log('val_loss_target', sum(outs15)/len(outs15))\n",
        "\n",
        "    self.log('val_acc_all inten', sum(outs14)/len(outs14))\n",
        "    self.log('val_loss_all emo', sum(outs16)/len(outs16))\n",
        "\n",
        "\n",
        "    print(f'***offensive f1 at epoch end {sum(outs)/len(outs)}****')\n",
        "\n",
        "    print(f'***val loss emotion at epoch end {sum(outs16)/len(outs16)}****')\n",
        "\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "      name,lab,txt,img,intensity,t1,t2,t3,t4,t5,t6,t7 = batch\n",
        "      name = batch[name]\n",
        "      lab = batch[lab]\n",
        "      txt = batch[txt]\n",
        "      img = batch[img]\n",
        "\n",
        "      intensity = batch[intensity]\n",
        "\n",
        "      t1,t2,t3,t4,t5,t6,t7 = torch.unsqueeze(batch[t1],1),torch.unsqueeze(batch[t2],1),\\\n",
        "      torch.unsqueeze(batch[t3],1),torch.unsqueeze(batch[t4],1),torch.unsqueeze(batch[t5],1),\\\n",
        "      torch.unsqueeze(batch[t6],1),torch.unsqueeze(batch[t7],1)\n",
        "      gt_target = torch.cat((t1,t2,t3,t4,t5,t6,t7),1)\n",
        "\n",
        "\n",
        "      _,logits,logit_arou,logit_val,inten,logit_target = self.forward(txt,img)\n",
        "\n",
        "\n",
        "      tmp = np.argmax(logits.detach().cpu().numpy(),axis=-1)\n",
        "      loss = self.cross_entropy_loss(logits, lab)\n",
        "      lab = lab.detach().cpu().numpy()\n",
        "      self.log('test_acc', accuracy_score(lab,tmp))\n",
        "\n",
        "      np.save('multitask_logit_emotion.npy',logit_emotion.detach().cpu().numpy())\n",
        "      np.save('multitask_logit_offensive.npy',lab)\n",
        "      np.save('multitask_logit_intensity.npy',inten.detach().cpu().numpy())\n",
        "      print(f'confusion matrix intensity {confusion_matrix(intensity.detach().cpu().numpy(),np.argmax(inten.detach().cpu().numpy(),axis=-1))}')\n",
        "      print(f'confusion matrix offensive {confusion_matrix(lab,tmp)}')\n",
        "      best_threshold = np.array([0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5])\n",
        "      y_test = torch.nn.Sigmoid()(logit_emotion)\n",
        "      y_test = y_test.detach().cpu().numpy()\n",
        "      y_pred = np.array([[1 if y_test[i][j]>=best_threshold[j] else 0 for j in range(13)] for i in range(len(y_test))])\n",
        "      #print(y_pred)\n",
        "      total_correctly_predicted = len([i for i in range(len(y_test)) if (y_test[i]==y_pred[i]).sum() == 13])\n",
        "      self.log('test_loss', loss)\n",
        "      #print(total_correctly_predicted)\n",
        "      pred_e = y_test\n",
        "      return {'test_loss': loss,\n",
        "              'test_loss_target': F.binary_cross_entropy_with_logits(logit_target.float(), gt_target.float()),\n",
        "              'test_loss_emotion_multilabel': F.binary_cross_entropy_with_logits(logit_emotion.float(), gt_emotion.float()),\n",
        "               'test_acc':f1_score(lab,tmp,average='macro'),\n",
        "              'test_acc inten': f1_score(intensity.detach().cpu().numpy(),np.argmax(inten.detach().cpu().numpy(),axis=-1),average='macro'),\n",
        "              }\n",
        "  def test_epoch_end(self, outputs):\n",
        "        # OPTIONAL\n",
        "        outs = []\n",
        "        outs14 = \\\n",
        "        []\n",
        "        outs15 = []\n",
        "        outs16 = []\n",
        "        outs17 = []\n",
        "        outs18 = []\n",
        "        for out in outputs:\n",
        "          outs15.append(out['test_loss_target'])\n",
        "          outs.append(out['test_acc'])\n",
        "\n",
        "          outs18.append(out['f1 sarcasm'])\n",
        "\n",
        "        #print(outs)\n",
        "        self.log('final test f1', sum(outs)/len(outs))\n",
        "\n",
        "        self.log('test_loss_all emo', sum(outs17)/len(outs17))\n",
        "        #self.log('test_f1_all sarcasm', sum(outs18)/len(outs18))\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=5e-3)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "class HmDataModule(pl.LightningDataModule):\n",
        "\n",
        "  def setup(self, stage):\n",
        "\n",
        "\n",
        "\n",
        "    self.hm_train = t_p\n",
        "    self.hm_val = v_p\n",
        "    self.hm_test = te_p\n",
        "\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.hm_train, batch_size=64)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.hm_val, batch_size=64)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.hm_test, batch_size=128)\n",
        "\n",
        "data_module = HmDataModule()\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "     monitor='val_acc_all_offn',\n",
        "     dirpath='noemo/ckpts/',\n",
        "     filename='our-ds-ckpt-epoch{epoch:02d}-val_f1_all_offn{val_acc_all_offn:.2f}',\n",
        "     auto_insert_metric_name=False,\n",
        "     save_top_k=1,\n",
        "    mode=\"max\",\n",
        " )\n",
        "all_callbacks = []\n",
        "all_callbacks.append(checkpoint_callback)\n",
        "\"\"\"\n",
        "for i in range(1,14):\n",
        "  tmp_checkpoint_callback = ModelCheckpoint(\n",
        "      monitor='val_acc_all e{}'.format(i),\n",
        "      dirpath='noemo/ckpts/e{}'.format(i),\n",
        "      filename='our-ds-ckpt-best-emo-{}'.format(i),\n",
        "      auto_insert_metric_name=False,\n",
        "      save_top_k=1,\n",
        "      mode=\"max\",\n",
        "  )\n",
        "  all_callbacks.append(tmp_checkpoint_callback)\n",
        "\"\"\"\n",
        "# train\n",
        "from pytorch_lightning import seed_everything\n",
        "seed_everything(seed=123, workers=True)\n",
        "hm_model = Classifier()\n",
        "gpus = 1 if torch.cuda.is_available() else 0\n",
        "trainer = pl.Trainer(gpus=gpus,max_epochs=60,callbacks=all_callbacks)\n",
        "#trainer = pl.Trainer(gpus=gpus,deterministic=True,max_epochs=60,callbacks=all_callbacks)\n",
        "\n",
        "trainer.fit(hm_model, data_module)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "howvS8j2ST-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wZ0t51ZnSTSK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}